
00:00:01
right hello my name is David Wear I'm going to be talking to you about mq clusters uh some of the features and how you can use them to solve your problems start off with just a little bit of uh why you might get into clusters we won't go into the lowlevel details of configuration or things like that here um that can be another session uh then we're going to talk about like I said some of the features like how do you maintain availability of your services through using mq clusters um how do you root me messages um intelligently how do you make sure that your system works efficiently um and then finally we're going to finish up uh with a little bit about how mq clusters interact with a disaster recovery setup so why might you have a cluster so when you start off with mq things typically start off nice and simple maybe even a single Q manager you're just using a queue to put messages on um and then take them off again process them maybe send a reply back again so got maybe a client server application as


00:01:02
we've got here but soon you're probably going to end up doing rooting messages between multiple Q managers maybe that's because you've got multiple systems um or maybe because you've got to be able to uh rout between different locations so that's not too bad you have to join the Q manages together by defining mq channels pointing them at each other defining transmission cues and some remote Q definitions but it's it's not too bad to understand at this point but as you scale up you want to scale up your applications more and more instances of them potentially and then you've got more applications to bring into your system as well once you've got those extra systems possibly you want to have more Q managers to service them maybe for scale again maybe it's because you've got a larger estate a larger Network um of systems so we've got more Q managers to have the more Q managers we have to inter uh connect them with more mq channels that we Define to each Q manager pair that needs to talk and then we need to Define all the extra definitions transmission cues remote CU


00:02:04
definitions those sorts of things so things are getting a bit more complicated here and then more and more applications come along over time you lose track of some of them what some of them are doing who they need to be able to talk to where they need to get messages to and from and then obviously more Q managers come in as well so you can see an mq estate can quickly become quite unmanageable from that point of view unless you keep a tight control over what your interactions are between your different Q managers and therefore which channels need defining and which other resources so that's where an mq cluster comes into its own mq cluster resolves down to every ke manager effectively needing two channel definitions it needs a reer channel um to say this is how everybody in the cluster is going to talk to me and a sender channel to bootstrap you into one of the full repository Q managers so once you've got that set up you don't have to worry about um remote Q definitions specific channels between any two pairs of Q managers because the cluster will work that out for you automatically on the Fly and create them for you so that you can use them without even realizing they're there so that's


00:03:08
sort of the first benefit of a cluster is simplification of your mq network but if we zoom in on a particular application there are other features of an mq cluster which you can benefit from so here we've got this request reply application we've got a client sending in requests um goes to a VI for um a service application and it processes that sends a response back and that goes back to the client so that's nice and simple two key managers in this picture but we might want to scale up that application we might want to do worklow balancing so if we have multiple instances of that service application what we can do is we can define an extra Q manager we can Define the same Q as we had of q1 on that first Q manager on the new Q manager now we've got two instances of the same Que in the cluster when you have multiple instances of the same que I it's got the same


00:04:09
name any Q manager that's sending messages to it that's going to workload balance messages by default across those multiple instances so now when a client sends in requests these purple messages and sends replies back you can see that the purple request messages are getting worklow balance across the two instances of that q1 so here we get workflow balancing so now we've uh scaled up the number of key managers that can process the messages which means you can distribute out your service applications um widely so that you can utilize more resources another thing you can do now is you can add in service availability if we lose one of these systems so we've lost that top system we've now still got one instance left in which case the Q manager over on the left is going to detect this as we'll see later and he's just going to send his requests down to that bottom key manager so we've instantly got availability in that if we've lost one instance of it all the new messages are going to get rooted through to the


00:05:12
remaining instance so we'll we'll look at these in more detail as we go through so we focus in first on availability that last one so we're back to the same picture as we had before but you need to understand where messages can get stuck if you have an availability problem what situations do we need to consider where a messag is going to get stuck in my system and how do I handle those so we see what we did before we've got our Target cues we've got the request Q over on the right we've got our reply Q over on the left and then you can see the dotted cues are cluster transmission cues so these are transmit cues where messages are temporarily going to be um stored while they're being transferred across the channels between these C managers so as a message goes through the system it might be stored on a transmission queue it might be stored on a Target queue before it's taken off by the service or when the reply comes back taken off by the client in all those situations what's being highlighted are all the


00:06:15
points where a message might get stopped in its tracks if we get an availability issue at any point in time so we need to consider how do we get make sure that any messages that get stuck in any of these positions how do they get processed and that depends really on the failure so if we first look at what happens if we lose um a q manager or or a whole host so we lose that c manager on the top now we've possibly got some messages stuck either in the transmission queue about to go from a client's queue manager over to the right top Q manager so there's some messages stuck or we've got some messages stuck on the KE manager itself waiting to be processed by the service or even reply messages waiting to come back to the client first thing that's going to happen though if we lose a q manager the service is going to go down that service isn't going to be connected to a q manager anymore so that's going to come unavailable if we first look at the messages on the left that are sat on a transmission queue waiting to be sent to


00:07:21
that top right hand Q manager those messages obviously can't be transmitted about Q manager because the Q manager is not available the channel is not going be available however the Q manager on the left detects this because the Channel's gone down and knows that those messages could potentially go to the bottom right hand Q manager because that Q manager hosts the same que now it can only do this if those messages have not been bound to the top right hand Q manager so you may have noticed at mq when you open a queue um you specify which bind type you want to use or leave it up to the def bind option on a queue the default bind type and that could be bind on open um bind on put or sorry uh not not bind in those situations if we bind on open to the queue we're saying all my messages that I send under this open need to go to one


00:08:22
instance of the queue which means if I do that at the start of sending messages and I pick the top left hand Q manager sorry the top right hand key manager then first message you go to that key manager and the next one and the next one and the next one they'll all go to the same one if that Q manager goes down halfway through me sending messages mq is not at Liberty to reroot any stopped or stuck messages to the bottom right hand Q manager because you've said we need to get them all to the same place so those messages we can't reallocate we have to leave on the transmission queue until that Q manager comes up if you said bind not fixed that means you don't mind which Q manager these messages go to therefore if any messages are stuck on the transmission queue when we lose a q manager the Q manager on the left can detect that and go well those messages were going to be going to the top Q manager now because they're bar not fixed I can send them to the bottom Q


00:09:23
manager instead so those messages can get transferred across and processed by the service so if availability is important to you one thing you need to consider is about bind type try not to bind on open if you bind on open you've locked yourselves in and possibly reduced the availability next thing to look at is those messages that are locked on that top leftand Q manager in mq if a q manager is unavailable typically the data or the messages on that Q manager are locked away until you bring that Q manager back up and running again the only really is shared cues on on zos in which case you have other other options available to you but if we if we ignore that for now and this is just a normal Q manager the only way to process those messages is nothing to do with mq clusters it's all about getting that Q manager back up and running as quickly as possible now that could be using an ha cluster it could be using multi-instance Q managers it could just be going in manually and restarting the Q manager or restarting the system


00:10:27
obviously each of those have um different things to consider and also different speeds of getting up and running again but the essence here is just to realize that any message is locked on that particular Q manager you need to get that Q manager up and running somewhere even if it's not in the same place on that note if you're say using multi-instance Q managers which you may be familiar with that means that Q manager can run on um of of two possible different systems those two different systems have different IP addresses if you're in an mq cluster you're quite welcome to use multi-instance key manager for any key manager in your cluster you just need to make sure that the channel definitions your re receiver Channel um with its uh con name in references both of those IP addresses both of those host names right once we've got a q manager back up and running those messages can be processed just as if it had never um failed but the first off any messages waiting to be transmitted they can go so the replies are off but the messages on the actual


00:11:29
um request q that service needs to be up and running again so you need to make sure that that service is sat there trying to reconnect or maybe you're running inside an application server or something like that where um the JMS resource adapter is already doing that reconnect Logic for you but the point is that service needs to reconnect and then it can process the messages and they can get on their way again so this is a combination of yes using multiple Q managers with multiple instances of the same Q to allow you to rot messages to an available instance but you do also have to get any down Q manager back up and running for you to be able to process any one particular message so that previous one was looking at what happens if we uh if a q manager is shut down for any particular reason what happens instead if it's the service application here that goes down if we're processing messages as we were before quite happily doing that and then we lose the instance of the service


00:12:33
but the Q manager stays up and running what's going to happen in this situation well actually the leftand Q manager isn't aware that the service has gone down because it's basing its liveness effectively on the channel connectivity between the left-hand Q manager and the two right hand Q managers and that is still up and running and quite healthy so when messages come through it's going to continue to workload balance them and all the messages go into the top key manager they're just going to start to build up because no one is processing those messages eventually you're going to fill the que which may start filling uh uh dead letter cues it might bring channels down um it might do all host of nasty things to your system so how do we solve this particular problem in mq clusters if we roll back get it back to how it was before we're going to need an an additional um setup here we provide a sample monitoring service that can sit and monitor your cues in your cluster


00:13:39
now you deploy it to each of your que managers where you host these cues um the service is called amq CLM you'll find it in our samples our tools directory of your installation and you'll also find it in the mandels and documented what it does but I'll just show you here it's basically sat there checking that those cues are currently being serviced the way it does it is it really just checks to see whether an application is connected or not and it has that Q open for input I.E it's getting messages off it it's not um smart enough to detect when one goes slower than the other it's not smart enough just detect um when the cues start to fill up because that Services become unresponsive we provide it as a sample so if you want to um put in extra logic like that into your system then you're quite welcome to do it but the simple one is just to detect whether that application is connected or not so it's sat there monitoring this and what it does is if it ever sees that that application is disconnected or


00:14:42
in fact reconnected it modifies the cluster configuration of those cues that goes out to a full repositories and then goes out to any of the que managers in the cluster that needed by modifying that um cluster configuration as we'll see in a minute it controls the worklow balancing algorithm which means it changes the rooting of those messages so let's have a look at what actually happens we're sending messages as we were before we've got rid of replies we're sending the messages and we lose our service now we're still sending messages so there's a chance some messages are going to slip through and arrive on that top Q manager even when that service is not running because the monitoring service is just effective polling or a period it's checking the state but let's say it's now checked the state it detects that the application is not connected it's going to do a couple of things first off it's going to modify the configuration of that que it's going


00:15:44
to say this Q from a cluster point of view is now not as attractive as that bottom Q which means now any further messages sent in to the left hand Q manager based on that configuration is always going to prefer to send messages to the bottom Q manager it's using an attribute on um the cluster Q called cwl um prty that's cluster workload priority if you're familiar with it but by modifying that dropping down the priority of the top one leaving the bottom one high means that messages will actually go to the bottom one over the top one so that deals with all new messages coming into the system what about those messages that are sat there on that Q that slipped through before we detected it well we can also run this monitoring service in a mode that says actually if any messages do get through and I don't have a service attached I want you to pick those messages up and I want them to drive them back through the system driving them back through the system


00:16:45
means we're actually going to send them down to that bottom instance of the queue because we know that one's got a Service attached so here we can run a system so that if you lose one of your services we'll also make sure that we intelligently rot the message mes to the remaining services and we'll also pick up any ones that slip through the net and get them down to that bottom key manager this solution is good if those services are running in a sort of steady state mode so they're normally connected they're normally attached and only in the event of failures do they detach so it's not good where these services are coming and going over time if they're coming and going over time then the monitoring service would have to try keep up with that and that could actually put undue stress on the on the clustering infrastructure of being able to keep up with those changes so we've looked at service failures and service side failures let's look at what happens with client


00:17:45
failures so because this is also a part of the solution we could lose that under on the left in which case the client can't attach anymore so this again isn't particularly specific to clustering but basically you want to have multiple points that that client can come into and now it can um choose whether it normally prefers one over the other or it's randomly picking between the two for workflow balancing now the thing from a clustering point of view is no matter which one you pick so we're going to pick that top one when it sends messages through they could get worklow balance to either of those services but when a reply message is sent back it knows which one to send it back to because it wants to send it back to the same ke manager that the client is attached to so typically and you can do various configuration things and um application things to override this Behavior but typically when a message gets sent through sorry when a message gets sent through then it's going to um tell the


00:18:48
receiving Q manager where that message came from and that's where the reply is going to go back to right let's move on to to uh another situation which referring to as local dep location dependency so this is a scenario we've got two systems um say we've got one in the US One in Europe so they're geographically dispersed we've got effectively the same service running um in two different data centers New York and London uh messages can be processed coming in from other key managers that are distributed across the globe maybe and other Europe European countries other locations in in America and they could be processed by any one of those four um Services instances of a service running on the right hand side however really we we're the opposite sides of the Atlantic so what do we want to do we really want to get messages processed to our local data


00:19:49
center that gives us um improved performance lower latency so that's our normal running system however if we have a problem with our data center so we have a problem with London all the systems uh go down what do we want to do in that situation rather than just saying those Q managers in Europe sorry you can't process messages anymore we want to root those messages via our remaining Data Center in New York okay it might take a little bit longer but it is actually still going to result in those messages being processed so that's what we want to achieve so how do we achieve it with a cluster there are a couple of ways to set this up let's first of all look how would you do this in one cluster so we've simplified the picture a bit and we've overlaid a single cluster because once you've got a cluster you've sort of given up control over where your messages are going to go to so this is a way of putting some of that control back in again so we're going to do um this preferred routing using um some aliases to map the


00:20:56
messages to and we're going to use the cluster work load priority um attribute again we saw that a minute ago um in use when we were talking about um service availability but here we're going to use it again so we Define our Alias we've got a request queue defined in New York and in London we Define an alias called uh nyq so that's our New York Q pointing to that request q and this we're going to put into this one Global cluster and we give it a high priority as well then we Define the same Alias called nyq so it's a New York Q Alias but this time we Define it down on the London Q manager which is slightly strange but you're going to see what we do here we put it into the cluster as well but this time we give it a low priority so we've got a high priority version and a low priority version then over on the left hand side we're going to Define ourselves a local


00:21:58
alias from the queue that the client is sending messages to and we're going to map that on to Target the nyq so this is on a us or American Q manager over on the left hand side so we've got appq is pointing to nyq now in this mode when both the New York and the London Q managers are running a client is going to send a message to appq it's going to be mapped to nyq it'll see both instances of nyq one of them will have a priority of n one of them will have a priority of four in that situation it will always pick the priority 91 if it can get a message there so if a channel is up and running between the Q manager on the left and the New York Q manager it's going to say well that's got the highest priority and I can get to it therefore I'm going to send my messages to it and it's going to ignore the priority for ones unless it can't get messages to the priority 9 New York Q manager in which case it'll say well I can't get to the


00:23:01
nine what's my next available best priority next best priority and it'll come down to this London one at priority 4 and it'll start rooting messages straight to London instead of the unavailable New York Q managers so that's how we do messages in the US then we set up the mirror configuration for a London Q Alias where we put a high priority down on the London Q managers and a low priority up on the New York ones and then any of our European Q managers over on the left side their Alias for appq instead of being to nyq is to lq so that's actually going to send messages to the Alias which has got a high priority down in London and a low priority up in New York and this is how you can have the same application running on a on a US system or a European system it can address messages to the same Target que which is app Q in its case not knowing where it's running and then the infrastructure is going to root the messages intelligently to its preferred


00:24:07
location we could do this with multiple clusters instead of a single cluster um there's nothing much between the two from a performance point of view or a runtime point of view it really comes down to um your style how you prefer to set up your systems also maybe how many cues you're rooting the previous uh uh example needed aliases uh set up for each individual que this model here which um if you get to the actual uh material slide material you'll find some notes information on SlideShare tells you how to configure this overlapping cluster solution using channels and we use the same priority the clwr priority but this time we use it on the channels rather on the cues in which case we prefer to send it based on the channel availability so this one it's a bit more complicated from a point of view if you need two clusters but it's a little simpler in execution in that you don't have to add in extra aliases every single q that you want to root in that


00:25:12
way so it just gives you choices right next we're going to talk about avoiding interference so what do I mean by that so when you've got a cluster you can sort of think of the transport of a messages is going through a big pipe where you have little control within a cluster of that pipe or transferring those messages so if you've got a complicated system lots of applications um doing different things with different types of messages so you've got some real time messages you got some some big data transfer messages uh you got some audit events each of those different types of messages need to get through at different speeds with different implications of if they don't um and some you just want to get through eventually and maybe they're going to be big and bulky and you realize that those are going to interfere with getting the the smaller quicker messages through now if you're doing that over a general purpose set of Q managers where all these applications can connect into and those Q managers are in one big


00:26:17
cluster like I said it's a bit like a pipe all the messages are going to just get um mapped onto their target cluster cues it's going to be down to the cluster which has got a CH def finded for each Q manager and it's going to use that channel to send those messages over so they're all going to mix up on the channel and some are going to get um blocked behind others which may not be what you're after even if you define multiple channels um for a particular Q manager within a cluster which you can do multiple cluster receiver channels for the same cluster then you still haven't got any control at that point over which one mq will pick to send each individual message so how do you get around this one way is to instead of having one cluster that all these Q managers are a member of Define multiple clusters so we've got three clusters defined here each of the Q managers is in all three of those clusters any one Q manager can be in as many clusters as it want you just have to define the different channels um to say which clusters they a member


00:27:22
of so now you're in multiple clusters don't use cluster name lists to share channels across those clusters have a separate channel for each cluster so each Q manager defines three cluster receiver channels and says um each of them is for a different cluster now when you define your cues to send your messages to you can pick which cluster you want to advertise each of those cues in so your real-time message cues they would go in one cluster your batch message um cues would go in a different cluster and your audit event messages would go in a different cluster which means when messages get targeted to them they'll get mapped to the queue which maps to the cluster which ultimately Maps down to which channel to use so by having those multiple clusters you've now got some way of categorizing your cues based on what type of messages they are and therefore mq can rout accordingly over the correct channels let's have a look at another type of interference that you can see um in mq clusters if we look at this


00:28:27
scenario it looks pretty similar to the scenario we had before where we've got um client sending messages to service applications but here we've got two different applications um we've got application one and application two and over on the right you can see that application one where we've got service one processing the messages is doing that over two instances of q1 whereas service 2 maybe that's a smaller application is only going to be doing it uh for one instance of a que and that's only going to be on one of the Q managers so we've got a an unbalanced of cues receiving those messages now applications are sending those messages in randomly for the different cues and receiving on the other side that's all very well apart from the workflow balancing is actually done at the channel level rather than the individual Q level so what does that mean so say we sent in 100 messages for application one and 50 messages for application two now they're all going to go through


00:29:29
that leftand Q manager so we've got 150 messages and that's can see that it's going to be able to send messages to the top uh right hand Q manager and the bottom right hand Q manager and it's going to try hard to balance them evenly across the two channels that it's going to be using which means it's going to manage to balance 75 messages maybe on average across those two instances those two channels which means receiving Q managers each receive 75 messages so nice and balanced at this point however if you then look at what's actually arrived on the individual cues to achieve this 50 of those messages on that bottom Q manager must have been going to Q2 because they're the only place that those messages could go to which means you've ended up with a split of 25 remaining messages have gone to q1 on the bottom right hand Q manager but 75 messages have gone to the top instances of q1 on that top Q manager so for q1 you may have hoped for a 50/50 split but what you've actually ended up


00:30:34
with is a 7525 split because mq has been trying to balance the channels rather than the individual Q's again how do you get around it well really to get around this what you'd need to do is you'd need to separate out q1 from Q2 um a cluster level so you have two different clusters if you've got two different clusters you can have separate channels as we saw on the previous slides if you got separate channels the leftand Q manager will balance based on the channels and then you'll get a nice even split depending on which cluster you put your different cues into or if you really want to get into it and um you could write yourself a cluster workload balancing um exit on those Q managers to have a bit more sophistication in it but that comes down to logic specific to your implementation while talking about um transmitting messages let's touch on cluster transmission


00:31:36
cues from day one sending messages um from any one Q manager to any other Q managers in a cluster or different clusters was very simple each Q manager had a single cluster transmit Q so that was system cluster transmit Q all the messages going down any cluster channels we go on to that transmission queue and then off again to the different channels this poses a number of um uh complications or problems in that it makes it hard to separate out your message traffic we've been talking about interference there on the channels but how do you separate them out on transmission cues for example if you've got one Q manager and your cluster goes down and we've got messages being sent to there could those messages that are going onto a transmission queue um back up to a point where the whole transmission queue is now unavailable for any new messages going to any member of any cluster um also how do you detect those sorts of situations how easy it is it for you to be able to um see the messages are building up for a


00:32:38
particular Channel um rather than just building up in general typically people do this looking at the depth of of transmit queue depth of messages on a transmit queue it's harder to do where that transmission queue is being used for multiple channels similarly for monitoring also controlling different limits um uh different um Behavior for the different transmission cues finally performance can be um impacted by a single transmission queue if you're really really hammering it if you're really really sending in High um high amounts of say non-persistent messages then there can be some contention on that single que just trying to process those messages but it's not normally a primary concern but that we have seen that as in some situations so in version 75 on distributive platforms are most of platforms and in version 8 on on zos and ibmi we've allowed you to split that out into multiple transmission cues so that now different messages for different


00:33:38
channels can go onto different transmission cues allowing you to improve how you monitor um how you manage it and potentially your performance so let's quickly just look at how you do this you've got two modes one is an automatic mode where you basically say on the que manager there's a new attribute of CLX Q so of a default cluster transmit Q if you set that to channel it's saying for every individual cluster um sender Channel that I create on this Q manager I'm going to use a different um transmission queue so that's ones whether either you manually configure the cluster transmit queue say it's to a full repository or it's autogenerated on the basis of that this Q manager needs to send some messages to another key manager in the cluster so we've got Channel a comes along at which point the left hand Q manager automatically will create you a channel a transmission queue called system cluster transmit and then the name of the channel and that's where all the


00:34:40
messages for that single channel are going to go on to and any additional channels that come along each get automatically their own transmission queue and the que manager will clean those transmission cues up as well when they're no longer needed so that's the automatic mode obviously if you got lots and lots of you'll end up with lots and lots of individual transmission cues alternatively you can have a bit more control over it and you can manually say which channels are going to use which transmission cues so a bit like you would in a non-clustered channel way you do this on the um sending end of the channels on that Q manager on the leftand side you define a local que of usage xmit Q so as if it was a normal transmission queue but you set the attribute of clch name so cluster channel name and here you can give it the name of a channel that you want it to use specifically for this transmission queue or you can give it a


00:35:41
pattern using asterisk as the wild card at any point in it to say actually any channels that match this naming convention are going to use this transmission que so say we set up two transmission cues called the Pink's trans que and the green transmiss q and we set them up to say we either want um uh all channels that start green to use one and all channels that start pink to use the other now if we've got a good naming convention for our channels we our clusters and say we've got a green cluster and a pink cluster then all our channels are named accordingly and this simple configuration means that all the channels for each individual cluster we're going to use a separate transmission queue so you haven't got a separate transmission queue per channel here we're using the pattern matching to group them together based on their effectively based on their cluster name worth pointing out that any channels that don't fall into that name and Convention then fall back to this


00:36:42
default um cluster transmit queue setting on the que manager do I then use the single system cluster transmit queue for all the ones that don't match or do I automatically generate a unique one for all of them instead so you can mix and match these two behaviors right finally talk a little bit about um disaster recovery so it's not about using mq clusters to give you a Disaster Recovery Solution it's more what do you have to um consider if you're using mq clusters and you have a Dr setup everyone seems to do it slightly differently um some things are easier than others some things are more complicated um to do but you can typically get mq clusters to fit into most of these setups we're just going to have a look at those um as we go through first one synchronous replication so we've got two data centers over here on the right data center one and data center two and we're doing synchronous replication between the two now synchronous replication effectively means um anything any fail


00:37:49
over from data center one to Data Center 2 is a bit like you turn it off and then you turn it back on again there's no loss of data um there's no Gap in the knowledge of these systems because they're synchronously being replicated so if we do have a failure and we fail over to Data Center two it's a bit like we're bringing up the same system so here the only thing to consider if we've got Q managers over on the left which aren't inside those data centers which are within clusters that are communicating with those Q managers in the data center they just need to be able to connect to them now that might be using um some Network layer that switches V P um so that it goes from data center one to Data Center 2 that's the most typical thing you could be using comma separated lists a bit like you would for multi-instance Q managers to say um all these Q managers could be running either in a location data center one or data center 2 but that's probably less likely the point is this is quite simple um to do because all you're doing is really um moving the que Managers


00:38:53
from one place to another there's no loss of knowledge or state in doing that moving which is different to the asynchronous replication that we're going to look at now if we got the same setup but this time we're doing asynchronous replication um so if it's asynchronous replication that means data center 2 is slightly behind data center one it could be very far behind depending on how you're asynchronously replicating or it could be as little as as a few milliseconds um that these Q managers are the knowledge of the que managers when they're running a data center one R compared to the information held or replicated across the data center 2 but it does mean that if data center one goes down and data center 2 comes back sorry comes up and starts running the same set of Q managers you've effectively jumped back in time all the messages that are on those on those Q managers have


00:39:53
potentially um moved backwards um for the Q that you're using in your system you might have lost the most recent updates or you might um have adding messages or even taking messages off them so that's something for you to consider from a um Dr point of view but it also affects clustering because cluster state is data held inside these Q managers it happens to be messages on qes as well but that's sort of irrelevant the point is is each Q manager in a cluster holds some State about what it knows about in the cluster and by moving from data center one to Data Center 2 with asynchronous replication that state could have gone back in time now if it's gone back in time you need to somehow bring them up back up to date with the other Q managers in the cluster the ones that haven't failed over the way you do this is to use refresh cluster now we normally warn against using refresh cluster because you only really need to do that in um in light of specific


00:40:55
problems where possibly um Q support actually advises you to do it in this situation where these Q managers have jumped back in time you have to use refresh to renew all the information they know about all the information that they push out to the system so you have to do that for every Q manager that has failed over in this system also it's worth saying when you fail back to Data Center 1 potentially from data center 2 you got to do a refresh as well to bring everything back in line again back in the old data center so you must do this cluster refresh now in the picture we got drawn here we've got a full repository that's failing over now with a full repository that fails over with asynchronous replication you've still got to do this refresh if you've got to do a Refresh on a full repository that's effectively destroying everything it knows so you've really lost the benefit of having a full repository there because it's going to


00:41:57
start a fresh it's going to go back to sort of day one not know anything about the cluster and have to learn all over again so maybe rather than failing over your full repository Q manager just rely on the fact that you have two different full repositories you keep them separate and those two different full repositories are effectively your Dr solution so how do we do that well maybe if we look at um in the green boxes here that's what you're actually replicating and rather than having the full repositories in that set move the full repository out of the replicated data you can leave it in data center one and you might as well take advantage to take the other full repository and put that in data center 2 now here A and B are normally running in parallel continually so they're both up to date but if you lose data center one completely You've Lost full repository a now you've still got full Repository B


00:42:58
if you got full repository B your system can still function quite happily without that first full repository also you've now got the benefit that you don't have to inue a Refresh on that full repository when you do your failover so you avoid that complication finally we'll look at a Dr situation where you don't do replication now um some may argue whether this is Dr or not but a lot of systems are set up in this way basically it's a way of being able to run all your services in your different data centers even if you don't fail over the individual messages the actual data that's being passed around them in this situation it's very very similar to that New York London scenario that we looked at earlier in that we have different Q managers running in the different data centers um but for onez data center 2 they're not normally in use because we're using something like cluster workload priority that we used before or we could be using cluster workload rank as well now rank


00:44:04
is very similar to priority in that we'll always send messages to the higher ranked instance of cues or channels within a cluster the difference is is that priority um routs dependent on whether the channel is available um to the different targets where you have different priorities rank doesn't take the channel liveness into account so if you're using rank you would actually effectively manually do a switch from data center 1 to Data Center 2 by switching the rank of your relative Q managers priority it would do it dynamically based on detecting that the channels have gone down now some people want to be able to dynamically rout um in the event of a failure other people want to have more control that's where you might want to use cluster workload rank instead bearing in mind that in this solution no matter what you use any messages that are on the Q managers in the fail data


00:45:05
center those messages aren't going to be available until you bring those systems back up and running and that brings me to the end so we've gone through the different aspects how you can use clusters to solve some of your problems hopefully that's been of use to you um thank you very much for

