
00:00:02
hi i'm just going to do a quick demo of the new mq availability model this is called native ha we just released it in 922 so that's in march 2021 as an early release not quite there for production yet we're busy working on that in the background but we've got it out there so we can get into your hands and you can have a try with it so what i'm going to be showing you is how that works um and it's something that works at this point in time inside openshift so i have an openshift deployment um this happens to be in ibm cloud using the managed open shift environment um it's deployed across multiple availability zones for sort of the best possible availability and what i've installed into that is the the operators needed to sort of manage and deploy mq so i have ibm mq operator here so if i click into that then this is the way that i can very easily if you're not familiar


00:01:02
create and manage my queue managers my runtime my my messaging runtimes so i'm going to create an instance which is going to create a key manager very very simple i can just give it a name so if i say demo native ha i need to accept the license as we always do then for this particular situation i need to pick a particular license it's all right there's instructions of which you need to to use this is just because i'm showing you the the new capability and that new capability like i said it's not there for production yet so you have to pick the right license to to allow you to use it accept the t's and c's right so this is like a standard deployment of the queue manager a very very simple one so now i'm going to tweak it slightly so that we get the new availability and capability so if i go into the queue manager i can pick availability and from there i picked native ha simple as that that's all i have to do to enable the availability the only other thing i need to do for this particular deployment is i need to


00:02:03
switch for storage because this one was set up like i said is a very very simple one this quick start one which i'm modifying then i'm basically going look rather than ephemeral storage which means if the queue manager restarts it disappears i want persistent claims i want persistent volumes underpinning this q manager because if i have a failure and this is highly available i obviously want the messages that were in the queue manager to be available wherever it restarts and i'll explain how that works in just a minute so once i've done those two things i can click on create and it's as simple as that that is now deploying out um a new queue manager with everything it needs to be able to run both your applications to connect in and to run in openshift so it's deploying all the services um the routes and the storage to get up and running so that's going to take a take a few minutes so while that's going on what i'm going to do is i'm going to flip to one i prepared earlier so we've got a single instance q manager here now the single instance q manager


00:03:03
is the very very simple one now a very very simple one if we go in and look at resources and i look at the stateful set and the pods i've got one pod so that single pod has got the container which has got the queue manager running in so mq fits very nicely into it into a container and it's very compact it's very small and isolated you can see here it's just sat there ticking over and it's a 0.


00:03:30
023 of a core so a tiny amount of resource just to have that messaging resource sat there ready and waiting for your applications to be able to send in you know as many messages as it wants to do to achieve and whatever it needs to accomplish now it's obviously not doing anything at the moment because i'm just using it here to demonstrate these sort of failure scenarios so with this single pod we are relying totally on openshift and therefore kubernetes to manage and maintain it so what that means is it gets deployed and we tell um openshift but we always want one instance of this and then it becomes openshift's responsibility to monitor it and make sure that if anything happens to it it'll restart it redeploy it and make sure it comes back up again and there's always one instance so that sounds pretty good doesn't it you don't have to worry from an mq point of view an application point of view about the availability of it so i'm going to demonstrate that and then i'm going to talk through the limitations behind that so if i


00:04:31
click delete delete pod it's not deleting the queue manager this is just saying right okay i'm going to stop this pod this container that's going to make open shift detect that and then go in and try and reschedule it so we're going to click delete and we can see from the status it's going down it's terminating and it's dropped from a ready state of one one as in one of one down to zero of one so that's saying it's now no longer ready as we'd expect because we deleted it and open shift is now going to be going well hang on a minute i was expecting to have one of those let me just check for sure that it's not there and then what i'm going to do is i'm going to go and reschedule it and i'm going to bring it back up and we'll run it so we're going to see that happening in the background while i'm talking so there's a number of problems with this approach one is you can steer still hasn't gone to ready so at that point the availability is not particularly fast now openshift and kubernetes therefore will do the recovery in certain situations which i'll come on to but from that point of view it doesn't


00:05:31
do it particularly quickly now it can sort of rely on that when the workload is lots and lots of instances of stateless applications because at that point what that means is um is that i could have lots and lots of instances normally running and if i lose a fraction of them then okay then i drop my capacity but i'm not dropping my availability and i'll get them back within you know 30 seconds 60 seconds something like that and you can see while i was talking um actually that pod came back so it was in that sort of in that sort of range of maybe a half a minute to a minute sort of time to do it but if you've got a solution where you really need to get to a particular thing which in this situation i've got a single queue manager now there are many reasons why you wouldn't want to have single queue manager you want to go for an active active deployment but i'm just keeping it simple here so in this situation you do want to get back to that q manager and you want it up as valuable as quickly as possible so the time it took was good but not brilliant so that's problem number one problem number two


00:06:33
is it'll actually only restart this pod for certain failure scenarios there are other failure scenarios where actually kubernetes and openshift will choose not to restart that pod now those examples are things like if i have a node failure so or i have an availability zone failure so i lose a particular availability zone at that point kubernetes doesn't really know whether the other side has really stopped or it's just lost contact with it and when you've got a stateful service running like mq is because it's looking after your messages then at that point it will err on the side of caution it won't restart it because it thinks well actually no i can't risk having two of these running in parallel because you only wanted one of them so i'll keep it down i'd rather have zero rather than two so it just doesn't restart it so that means from availability point of view the first problem was it's not very fast second problem is it doesn't always come back and you have to operationally go in and say no no i now


00:07:33
know that it's stopped and therefore can you restart it somewhere else the third problem which is also a very significant problem is around the persistent storage underneath the key manager that i showed you when i was creating one a minute ago now that persistent storage in this situation with this q manager which is deployed into this multi-az openshift cluster that storage would need to be available to this key manager no matter where it wanted to run so across all those different availability zones which means i've got to have a storage layer which provides me replicated storage consistent replicated storage across those different availability zones so that if i have a failure in one i've still got access to the storage elsewhere and i've got access to the storage elsewhere then i can restart it if i don't then the queue manager won't come back up so from that point of view i've got to have a storage layer deployed which provides me exactly what i need now that's actually not something you get with kubernetes or openshift that's


00:08:34
something that you have to layer on top so there are storage solutions out there so from an openshift point of view various ones you could plug in one would be openshift container storage and which we know works so from how mq wants it so from that point of view you could use that but it is an extra thing you've got to live with and manage and maintain and understand as you deploy out your mq runtimes so from our point of view it's definitely not ideal so if you've got those three particular problems which means this availability model is not the best one now what we've been doing with mq in the past is we've been saying well we can overcome the first two the speed of recovery and the recovering in all situations if you deploy mq as a multi-instance option at that point we would deploy two pods and those two plots pods would use shared storage so that sophisticated replicated storage which i told you about you would need that underneath it still and we would rely on that to do the replication


00:09:34
and now we'd be relying on it to provide us locking semantics so that we could detect a failure and restart so now the dependency on the file system is even greater and the way that the file system works um becomes more and more critical to how the queue manager works so we get better availability but we get a higher reliance on this file system and that like i said is not the best way because you then got to make sure that the file system is working exactly as you want it and you've got to look after it and maintain it so that said what i'm going to do is i'm going to skip back to my mq operator and look at key managers now while i was talking the one that i kicked off a minute ago or eight minutes ago when i was talking for quite a while um has started up so i could have used one that i created previously but i'll actually use the one that i've just created just to show there's nothing on my sleeve if i now go into resources there's a bit more resources now mainly because what you'll find if we go into stable set and look at the pods is we've now got three pods we've got a


00:10:35
single pot we've got three pods all running now now these are all actively running and what they're doing is they've each got now a persistent volume claim so they've each got a piece of persistence and that is where the mq data is going to be stored to now rather than relying on the file system to to have us for mq to see a single copy and the file system to do the replication of the availability mq is taking on that responsibility so each of these parts has its own piece of persistence and as updates come in some messages get put and got from from say the queue manager then those updates are being replicated synchronously replicated across the different pods which means at that point one of them could fail and another one could take over so similar sort of model to say that multi-instance model or rdqm if you're familiar with mq running say on a linux vm but it's all happening inside mq that replication is happening inside and that consistency across the replicas is happening as well


00:11:36
so with that you no longer have that dependency for that sophisticated storage layer this is using simple block storage read write once three separate instances of air across the three different pots now we're also following a very common sort of cloud pattern and implementation of how you get consistency across them so if you're familiar with consistency and consensus algorithms then you might be familiar with something called raft raft is a is an algorithm that defines how do i actually ensure that when i'm making all these critical updates those updates are consistent across multiple copies and how do i make sure that only the right copies are actually active and those ones are being written to and that's what we've pushed right into the heart of mq so what we have now is we have these three pods running um under raft in consensus


00:12:36
with a leader follower pattern so mq will automatically elect one of these instances to be the leader and that leader is where your connections are automatically routed into that's what some of the other resources under that q manager we're showing you that's how we automatically get the traffic in you don't have to find it as an application that's where the updates will go and then those will be replicated out to the other pods as you can see here so if we look at this they're all running because they're all sort of active and they're all i'm getting the the replication happening but one of them is the leader and it's the one that's marked as ready one one that means one of one i.e this one is the one where the traffic is going to go into so we know that pod zero is the active one there's the leader in this state so what i'm going to do i'm going to do the same demo i'm going to delete that one so it's going to go through the same cycle openshift will eventually detect it and it will eventually restart it but in the meantime we want to overcome we've overcome the


00:13:38
storage problem already um we want to then can we tackle the first problem the second problem which is the speed of recovery and recovery in all environments now that recovery across sorry all failures that one we can cover thanks to the consensus that i just talked to you about because we're no longer reliant on open shift to detect the problem so we know thanks to consensus and raft exactly when one is not available and we can make sure that that becomes unavailable if everyone else thinks it is so we know we've got consistency so then we're just going to see in the demo we're going to see the speed of recovery because now it's under our control so i'm going to do delete on that top pod so we can see it goes into terminating and you'll see the ready state goes down from zero to one and now the other two pods they're now detecting that this has failed and as we saw there if you just noticed it pod one has gone to ready which means it took what was


00:14:38
that maybe i don't know four or five seconds to detect that failure and come back up again so much much faster than relying on openshift and you have to take my word for it but those failures would be detected no matter what the um the problem was if we had a no failure and a z failure we would do exactly the same logic and exactly the same speed of recovery so hopefully that given you a little bit of an insight and and you can see what's going on from a native edu point of view um thank you very much

